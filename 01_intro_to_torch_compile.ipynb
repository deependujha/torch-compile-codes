{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Compile\n",
    "\n",
    "torch.compile is the latest method to speed up your PyTorch code! torch.compile makes PyTorch code run faster by JIT-compiling PyTorch code into optimized kernels, all while requiring minimal code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device='mps'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import warnings\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(f\"using {device=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## torch.compile basic usage\n",
    "\n",
    "Arbitrary Python functions can be optimized by passing the callable to torch.compile.\n",
    "\n",
    "We can then call the returned optimized function in place of the original function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3567,  0.1011],\n",
      "        [-1.6241,  0.3115]])\n"
     ]
    }
   ],
   "source": [
    "def foo(x, y):\n",
    "    a = torch.sin(x)\n",
    "    b = torch.cos(y)\n",
    "    return a + b\n",
    "\n",
    "\n",
    "opt_foo1 = torch.compile(foo)\n",
    "\n",
    "print(opt_foo1(torch.randn(2, 2), torch.randn(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can also use decorator to compile a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5304, -0.2428],\n",
      "        [ 1.2837,  1.6413]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.randn(2, 2)\n",
    "t2 = torch.randn(2, 2)\n",
    "\n",
    "\n",
    "@torch.compile\n",
    "def opt_foo2(x, y):\n",
    "    a = torch.sin(x)\n",
    "    b = torch.cos(y)\n",
    "    return a + b\n",
    "\n",
    "\n",
    "print(opt_foo2(t1, t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can also optimize torch.nn.Module instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4956, 0.7377, 0.2008, 0.0000, 0.2288, 0.6336, 1.4262, 0.0000, 0.0000,\n",
      "         0.4541],\n",
      "        [0.8629, 0.1461, 0.5833, 0.0000, 0.0544, 0.0000, 0.4984, 0.5596, 0.0000,\n",
      "         0.4246],\n",
      "        [0.5437, 0.0811, 0.0756, 0.0083, 0.8152, 0.2054, 0.2765, 0.0000, 0.0000,\n",
      "         0.0901],\n",
      "        [0.4975, 0.2585, 0.0000, 0.2944, 0.6465, 0.0000, 0.5874, 0.0357, 0.1657,\n",
      "         0.0000],\n",
      "        [0.2115, 0.2578, 0.0000, 0.4508, 0.0000, 0.0000, 0.4244, 0.0000, 0.0000,\n",
      "         0.3522],\n",
      "        [0.0000, 0.1485, 0.0000, 0.0000, 0.0000, 0.0000, 0.5496, 1.2266, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.7922, 0.2276, 0.0351, 0.0000, 0.0142, 0.0000, 0.4307, 0.4509,\n",
      "         0.0000],\n",
      "        [0.3721, 0.9828, 0.0000, 0.3258, 0.0000, 0.0000, 0.5848, 0.0000, 0.0000,\n",
      "         0.2040],\n",
      "        [0.0000, 0.0000, 0.0000, 0.5841, 0.0000, 0.0000, 0.0000, 0.9089, 0.0982,\n",
      "         0.2046],\n",
      "        [0.1742, 0.0000, 0.0000, 0.0000, 0.5193, 0.0000, 0.1036, 0.1628, 0.0000,\n",
      "         0.0000]], grad_fn=<CompiledFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(10, 100)\n",
    "\n",
    "\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = torch.nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.relu(self.lin(x))\n",
    "\n",
    "\n",
    "mod = MyModule()\n",
    "opt_mod = torch.compile(mod)\n",
    "print(opt_mod(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## torch.compile and Nested Calls\n",
    "\n",
    "Nested function calls within the decorated function will also be compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5304, -0.2428],\n",
      "        [ 1.2837,  1.6413]])\n"
     ]
    }
   ],
   "source": [
    "def nested_function(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "\n",
    "@torch.compile\n",
    "def outer_function(x, y):\n",
    "    a = nested_function(x)\n",
    "    b = torch.cos(y)\n",
    "    return a + b\n",
    "\n",
    "\n",
    "print(outer_function(t1, t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the same fashion, when compiling a module all sub-modules and methods within it, that are not in a skip list, are also compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2111, 0.0000],\n",
      "        [0.3138, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.2170, 0.0000],\n",
      "        [0.1764, 0.0000],\n",
      "        [0.0862, 0.1966],\n",
      "        [0.0000, 0.3254],\n",
      "        [0.0000, 0.3184],\n",
      "        [0.0594, 0.0000],\n",
      "        [0.2973, 0.0000]], grad_fn=<CompiledFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "class OuterModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inner_module = MyModule()\n",
    "        self.outer_lin = torch.nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inner_module(x)\n",
    "        return torch.nn.functional.relu(self.outer_lin(x))\n",
    "\n",
    "\n",
    "outer_mod = OuterModule()\n",
    "opt_outer_mod = torch.compile(outer_mod)\n",
    "print(opt_outer_mod(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also disable some functions from being compiled by using torch.compiler.disable. Suppose you want to disable the tracing on just the complex_function function, but want to continue the tracing back in complex_conjugate. In this case, you can use torch.compiler.disable(recursive=False) option. Otherwise, the default is recursive=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.4721, 5.8310])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Deependu/miniconda3/lib/python3.12/site-packages/torch/_inductor/lowering.py:1713: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def complex_conjugate(z):\n",
    "    return torch.conj(z)\n",
    "\n",
    "\n",
    "@torch.compiler.disable(recursive=False)\n",
    "def complex_function(real, imag):\n",
    "    # Assuming this function cause problems in the compilation\n",
    "    z = torch.complex(real, imag)\n",
    "    return complex_conjugate(z)\n",
    "\n",
    "\n",
    "def outer_function():\n",
    "    real = torch.tensor([2, 3], dtype=torch.float32)\n",
    "    imag = torch.tensor([4, 5], dtype=torch.float32)\n",
    "    z = complex_function(real, imag)\n",
    "    return torch.abs(z)\n",
    "\n",
    "\n",
    "# Try to compile the outer_function\n",
    "try:\n",
    "    opt_outer_function = torch.compile(outer_function)\n",
    "    print(opt_outer_function())\n",
    "except Exception as e:\n",
    "    print(\"Compilation of outer_function failed:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
